{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1mdmrgRkrgnuYP13WuvMPKy04jcLV_MJ0","timestamp":1672033510644},{"file_id":"1xi5iQqdXUtkazSdiJ1LjEu_wSy2GRv6O","timestamp":1672030545965}],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"markdown","source":["# LSTM의 `return_sequences`, `return_state` 이해하기"],"metadata":{"id":"00y_QhjBHD0g"}},{"cell_type":"code","source":["import numpy as np\n","from tensorflow.keras.layers import LSTM"],"metadata":{"id":"kIBjDIVnHNG1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_data = np.random.randn(1, 4, 5) # (Batch, Timesteps, Embedding dim)"],"metadata":{"id":"EZCBRLx2KJ61"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["case 1. `return_sequences=False`, `return_state=False`\n","* 제일 마지막 `hidden_state`만 반환된다.\n","* 이 옵션이 기본값"],"metadata":{"id":"v1XsiXRbKZcm"}},{"cell_type":"code","source":["last_hidden_state = LSTM(3, return_sequences=False, return_state=False)(sample_data)\n","print(last_hidden_state)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-j2ugN7WKkip","outputId":"3ea81dea-8944-4181-de8c-1e1c959d1b25"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([[-0.24535894 -0.27312386 -0.0538178 ]], shape=(1, 3), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["case 2. `return_sequences=False`, `return_state=True`\n","* `hidden_states` : 제일 마지막 state. `last_hidden_state`와 같다.\n","* `last_hidden_state` : 제일 마지막 `hidden_state`\n","* `last_cell_state` : 제일 마지막 `cell_state`"],"metadata":{"id":"rA1cDFo1LPex"}},{"cell_type":"code","source":["hidden_states, last_hidden_state, last_cell_state = LSTM(3, return_sequences=False, return_state=True)(sample_data)\n","\n","print(\"hidden_states : {}\".format(hidden_states.__repr__()))\n","print(\"last_hidden_state : {}\".format(last_hidden_state.__repr__()))\n","print(\"last_cell_state : {}\".format(last_cell_state.__repr__()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G5WYVC4nLsiT","outputId":"f5ade1c9-2318-4977-bcd1-a656af87b9b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["hidden_states : <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[-0.10527468, -0.12779412, -0.19543421]], dtype=float32)>\n","last_hidden_state : <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[-0.10527468, -0.12779412, -0.19543421]], dtype=float32)>\n","last_cell_state : <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[-0.17647648, -0.4138779 , -0.28452817]], dtype=float32)>\n"]}]},{"cell_type":"markdown","source":["case 3. `return_sequences=True`, `return_state=False`\n","* `hidden_states` 만 등장한다.\n","* 매 시퀀스의 출력값만 등장"],"metadata":{"id":"1wEKe5S9Moow"}},{"cell_type":"code","source":["hidden_states = LSTM(3, return_sequences=True, return_state=False)(sample_data)\n","print(\"hidden_states : {}\".format(hidden_states.__repr__()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ND3ph3wSNf7z","outputId":"e40391df-a6e0-42c4-e80a-54476406114d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["hidden_states : <tf.Tensor: shape=(1, 4, 3), dtype=float32, numpy=\n","array([[[-0.10560024, -0.14313582, -0.27681196],\n","        [-0.09291577, -0.25481808, -0.01195437],\n","        [-0.31565538, -0.2275723 , -0.07195552],\n","        [-0.51375395, -0.1825685 , -0.39207292]]], dtype=float32)>\n"]}]},{"cell_type":"markdown","source":["case 4. `return_sequences=True`, `return_state=True`\n","* `hidden_states`, `last_hidden_state`, `last_cell_state`가 모두 등장"],"metadata":{"id":"EJJSF3j3Nwwf"}},{"cell_type":"code","source":["hidden_states, last_hidden_state, last_cell_state = LSTM(3, return_sequences=True, return_state=True)(sample_data)\n","\n","print(\"hidden_states : {}\".format(hidden_states.__repr__()))\n","print(\"last_hidden_state : {}\".format(last_hidden_state.__repr__()))\n","print(\"last_cell_state : {}\".format(last_cell_state.__repr__()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"57qzgwwkOKzi","outputId":"9672f0d3-142c-4fff-f2a9-a25e68bce495"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["hidden_states : <tf.Tensor: shape=(1, 4, 3), dtype=float32, numpy=\n","array([[[ 0.00815869,  0.10324696, -0.12757036],\n","        [-0.03125606,  0.00970532, -0.17458802],\n","        [-0.05928417,  0.19194022, -0.3302095 ],\n","        [ 0.08037872,  0.49555954, -0.15872242]]], dtype=float32)>\n","last_hidden_state : <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.08037872,  0.49555954, -0.15872242]], dtype=float32)>\n","last_cell_state : <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.13251474,  0.63943154, -1.1759439 ]], dtype=float32)>\n"]}]},{"cell_type":"markdown","source":["# Seq2Seq 구현하기"],"metadata":{"id":"ojveX57vOU2P"}},{"cell_type":"markdown","source":["## Encoder"],"metadata":{"id":"-4-Ca_MHP21P"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","from tensorflow.keras.layers import Embedding, LSTM\n","\n","class Encoder(tf.keras.Model):\n","\n","  def __init__(self, vocab_size=2000):\n","    super(Encoder, self).__init__()\n","    # 레이어 구성 - Embedding Layer - LSTM\n","    self.emb = Embedding(vocab_size, 128)\n","    self.lstm = LSTM(512, return_sequences=False, return_state=True)\n","\n","  def call(self, x):\n","    x = self.emb(x)\n","\n","    # Encoder에서는 Context Vector만 만들면 되기 때문에 hidden_states에 대한 리턴은 필요 없다.\n","    _, h, c = self.lstm(x)\n","\n","    return h, c  # Context Vector"],"metadata":{"id":"M_CeZGFtP4MU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Decoder"],"metadata":{"id":"QfOrR1JtRLvV"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Dense\n","\n","class Decoder(tf.keras.Model):\n","\n","  def __init__(self, vocab_size=2000):\n","    super(Decoder, self).__init__()\n","\n","    self.emb = Embedding(vocab_size, 128)\n","    self.lstm = LSTM(512, return_sequences=True, return_state=True)\n","\n","    # lstm의 모든 시퀀스 마다의 softmax를 적용시키기 위한 dense 레이어 선언\n","    self.dense = Dense(vocab_size, activation='softmax')\n","\n","  def call(self, inputs):\n","    # decoder의 LSTM 호출은 데이터를 하나 하나씩 넣어주면서 timesteps에 의한 lstm 작동을 수동으로 해 준다.\n","    # inputs : 이전 스텝의 출력값, hidden_state, cell_state\n","    y, h, c = inputs # shifted_input, hidden_state, cell_state\n","\n","    y = self.emb(y)\n","                                                 # 이전 스텝의 hidden state, cell state를 강제로 넣어준다.\n","    y, h, c = self.lstm(y, initial_state=[h, c]) # initial_state : LSTM의 hidden_state, cell_state를 강제로 설정\n","    \n","    return self.dense(y), h, c # softmax 결과, hidden state, cell state"],"metadata":{"id":"BeBrzy49RVHb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Seq2Seq 구현"],"metadata":{"id":"KWBUTE8YT46F"}},{"cell_type":"code","source":["class Seq2Seq(tf.keras.Model):\n","\n","  def __init__(self, sos=\"<sos>\", eos=\"<eos>\"):\n","    super(Seq2Seq, self).__init__()\n","    self.enc = Encoder()\n","    self.dec = Decoder()\n","    self.sos = sos\n","    self.eos = eos\n","\n","  def call(self, inputs, training=False):\n","    '''\n","      inputs : feature, label\n","            : 훈련(training=True) 시에는 x, y를 동시에 받아와서 교사강요(Teacher Forcing)\n","            : 예측(training=False) 시에는 x만 받아와서 예측 수행\n","    '''\n","    if training:\n","      x, y = inputs # output, shifted input이 동시에 들어온다. [\"이것은\", \"사과다\", \"<eos>\"] , [\"<sos>\", \"It's\", \"Apple\", \"<eos>\"]\n","\n","      # context vector\n","      h, c = self.enc(x)\n","\n","      # y가 한꺼번에 들어간다. Teacher Forcing 수행\n","      y, _, _ = self.dec((y, h, c))\n","\n","      return y\n","    else:\n","      x = inputs\n","      h, c = self.enc(x)\n","\n","      # <sos> 텐서를 강제로 만들어 주기\n","      y = tf.convert_to_tensor(self.sos)\n","      y = tf.reshape(y, (1, 1)) # (1 배치, 1단어)\n","\n","      # 예측값을 저장할 텐서를 미리 만들어 놓는다.\n","      total_seq = tf.TensorArray(tf.int32, 64) # 최대 64개의 공간을 미리 만들어 준다.\n","\n","      for idx in tf.range(64):\n","        y, h, c = self.dec([y, h, c]) # 이 때 들어가는 y의 shape : (1, 1)\n","        y = tf.cast(tf.argmax(y, axis=-1), dtype=tf.int32)\n","\n","        y = tf.reshape(y, (1, 1))\n","        total_seq = total_seq.write(idx, y)\n","\n","        # y의 출력값이 eos라면 반복 중지\n","        if y == self.eos:\n","          break\n","      \n","      return tf.reshape(total_seq.stack(), (1, 64))"],"metadata":{"id":"P54gFPusT-sa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 훈련, 테스트 루틴 정의"],"metadata":{"id":"_Edr6nnrYAD_"}},{"cell_type":"code","source":["@tf.function\n","def train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_accuracy):\n","    output_labels = labels[:, 1:] # 제일 처음은 빼고 집어 넣는다. <sos>는 빼고~. 즉 <sos>가 없고 <eos>가 있고\n","    shifted_labels = labels[:, :-1] # <sos>를 포함하고 <eos>를 뺀다\n","    with tf.GradientTape() as tape:\n","        predictions = model([inputs, shifted_labels], training=True)\n","        loss = loss_object(output_labels, predictions)\n","    gradients = tape.gradient(loss, model.trainable_variables)\n","\n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","    train_loss(loss)\n","    train_accuracy(output_labels, predictions)\n","\n","@tf.function\n","def test_step(model, inputs):\n","    return model(inputs, training=False)"],"metadata":{"id":"6nq2jiCBWETW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"id":"4SccqM6QYUk1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0b8ffe8f-47d6-4ab2-874b-5d3b2d2cc41d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: konlpy in /usr/local/lib/python3.8/dist-packages (0.6.0)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.21.6)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (4.9.2)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.4.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from JPype1>=0.7.0->konlpy) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\n"]}]},{"cell_type":"code","source":["from konlpy.tag import Okt\n","import random"],"metadata":{"id":"MR-UV5TiYWAP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_file = 'chatbot_data.csv'\n","okt = Okt()\n","\n","with open(dataset_file, 'r') as file:\n","    lines = file.readlines()\n","    seq = [' '.join(okt.morphs(line)) for line in lines]\n","\n","questions = seq[::2]\n","answers = ['\\t ' + lines for lines in seq[1::2]]\n","\n","num_sample = len(questions)\n","\n","perm = list(range(num_sample))\n","random.seed(0)\n","random.shuffle(perm)\n","\n","train_q = list()\n","train_a = list()\n","test_q = list()\n","test_a = list()\n","\n","for idx, qna in enumerate(zip(questions, answers)):\n","    q, a = qna\n","    if perm[idx] > num_sample//5:\n","        train_q.append(q)\n","        train_a.append(a)\n","    else:\n","        test_q.append(q)\n","        test_a.append(a)\n","\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=2000,\n","                                                  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~')\n","\n","tokenizer.fit_on_texts(train_q + train_a)\n","\n","# 정수 인코딩\n","train_q_seq = tokenizer.texts_to_sequences(train_q)\n","train_a_seq = tokenizer.texts_to_sequences(train_a)\n","\n","test_q_seq = tokenizer.texts_to_sequences(test_q)\n","test_a_seq = tokenizer.texts_to_sequences(test_a)\n","\n","# 패딩\n","x_train = tf.keras.preprocessing.sequence.pad_sequences(train_q_seq,\n","                                                        value=0,\n","                                                        padding='pre',\n","                                                        maxlen=64)\n","y_train = tf.keras.preprocessing.sequence.pad_sequences(train_a_seq,\n","                                                        value=0,\n","                                                        padding='post',\n","                                                        maxlen=65) # 앞에 sos를 붙여주고, 뒤에 eos가 붙어있는 상황. 훈련 시에 sos가 붙으면 eos가 빠지고, eos가 붙으면 sos가 빠지기 때문에 65개\n","\n","\n","x_test = tf.keras.preprocessing.sequence.pad_sequences(test_q_seq,\n","                                                       value=0,\n","                                                       padding='pre',\n","                                                       maxlen=64)\n","y_test = tf.keras.preprocessing.sequence.pad_sequences(test_a_seq,\n","                                                       value=0,\n","                                                       padding='post',\n","                                                       maxlen=65)\n","\n","train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32).prefetch(1024)\n","test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(1).prefetch(1024)"],"metadata":{"id":"xZVbddWEYKwR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 모델 생성 및 훈련"],"metadata":{"id":"sU5FpVdVYttH"}},{"cell_type":"code","source":["# 모델 생성\n","model = Seq2Seq(sos=tokenizer.word_index['\\t'],\n","                eos=tokenizer.word_index['\\n'])\n","\n","# Loss, 최적화 정의\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n","optimizer = tf.keras.optimizers.Adam()\n","\n","# 훈련 평가방식 정의\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"],"metadata":{"id":"Zj2KVHUMY0uh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 학습 루프 작동 시키기"],"metadata":{"id":"K5iQ0HHHY_NG"}},{"cell_type":"code","source":["for epoch in range(200):\n","    for seqs, labels in train_ds:\n","        train_step(model, seqs, labels, loss_object, optimizer, train_loss, train_accuracy)\n","\n","    template = 'Epoch {}, Loss: {}, Accuracy: {}'\n","    print(template.format(epoch + 1,\n","                          train_loss.result(),\n","                          train_accuracy.result() * 100))\n","\n","    train_loss.reset_states()\n","    train_accuracy.reset_states()"],"metadata":{"id":"kP2pvLGyY97G","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4a0c0c6d-84fc-4f45-95f1-dec940f2e394"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 3.1125128269195557, Accuracy: 83.22368621826172\n","Epoch 2, Loss: 0.604878306388855, Accuracy: 90.92261505126953\n","Epoch 3, Loss: 0.5685924291610718, Accuracy: 91.0753402709961\n","Epoch 4, Loss: 0.5580309629440308, Accuracy: 91.09100341796875\n","Epoch 5, Loss: 0.5391191840171814, Accuracy: 91.09492492675781\n","Epoch 6, Loss: 0.5338164567947388, Accuracy: 91.1614990234375\n","Epoch 7, Loss: 0.5396810173988342, Accuracy: 91.1732406616211\n","Epoch 8, Loss: 0.5245455503463745, Accuracy: 91.19673919677734\n","Epoch 9, Loss: 0.5149692893028259, Accuracy: 91.2202377319336\n","Epoch 10, Loss: 0.5025860071182251, Accuracy: 91.31422424316406\n","Epoch 11, Loss: 0.49026569724082947, Accuracy: 91.5413589477539\n","Epoch 12, Loss: 0.4730362892150879, Accuracy: 91.61576080322266\n","Epoch 13, Loss: 0.4583468735218048, Accuracy: 92.1483383178711\n","Epoch 14, Loss: 0.4438988268375397, Accuracy: 92.33631134033203\n","Epoch 15, Loss: 0.43541452288627625, Accuracy: 92.42637634277344\n","Epoch 16, Loss: 0.4296739399433136, Accuracy: 92.53211212158203\n","Epoch 17, Loss: 0.421442449092865, Accuracy: 92.55168914794922\n","Epoch 18, Loss: 0.417848140001297, Accuracy: 92.5791015625\n","Epoch 19, Loss: 0.4126289486885071, Accuracy: 92.60260009765625\n","Epoch 20, Loss: 0.3983666002750397, Accuracy: 92.63392639160156\n","Epoch 21, Loss: 0.4022780656814575, Accuracy: 92.66133880615234\n","Epoch 22, Loss: 0.39279255270957947, Accuracy: 92.74357604980469\n","Epoch 23, Loss: 0.3895239233970642, Accuracy: 92.7827377319336\n","Epoch 24, Loss: 0.38511818647384644, Accuracy: 92.88846588134766\n","Epoch 25, Loss: 0.3784952461719513, Accuracy: 92.90021514892578\n","Epoch 26, Loss: 0.3765183985233307, Accuracy: 92.90805053710938\n","Epoch 27, Loss: 0.37181276082992554, Accuracy: 93.00203704833984\n","Epoch 28, Loss: 0.3696257770061493, Accuracy: 92.99028778076172\n","Epoch 29, Loss: 0.362833708524704, Accuracy: 93.15084838867188\n","Epoch 30, Loss: 0.3607279360294342, Accuracy: 93.15868377685547\n","Epoch 31, Loss: 0.3530157804489136, Accuracy: 93.2135009765625\n","Epoch 32, Loss: 0.35187071561813354, Accuracy: 93.3114013671875\n","Epoch 33, Loss: 0.3472776412963867, Accuracy: 93.33489990234375\n","Epoch 34, Loss: 0.3459809720516205, Accuracy: 93.33098602294922\n","Epoch 35, Loss: 0.3407450318336487, Accuracy: 93.43672180175781\n","Epoch 36, Loss: 0.33237606287002563, Accuracy: 93.48371124267578\n","Epoch 37, Loss: 0.32993176579475403, Accuracy: 93.46804809570312\n","Epoch 38, Loss: 0.3244112730026245, Accuracy: 93.56202697753906\n","Epoch 39, Loss: 0.32508227229118347, Accuracy: 93.55811309814453\n","Epoch 40, Loss: 0.3182162642478943, Accuracy: 93.58943939208984\n","Epoch 41, Loss: 0.31205877661705017, Accuracy: 93.7108383178711\n","Epoch 42, Loss: 0.30554071068763733, Accuracy: 93.79307556152344\n","Epoch 43, Loss: 0.3020336627960205, Accuracy: 93.87139892578125\n","Epoch 44, Loss: 0.29793262481689453, Accuracy: 93.94188690185547\n","Epoch 45, Loss: 0.29402589797973633, Accuracy: 94.043701171875\n","Epoch 46, Loss: 0.28759944438934326, Accuracy: 94.10636138916016\n","Epoch 47, Loss: 0.2851411700248718, Accuracy: 94.21208953857422\n","Epoch 48, Loss: 0.28461015224456787, Accuracy: 94.26300048828125\n","Epoch 49, Loss: 0.2731906473636627, Accuracy: 94.3961410522461\n","Epoch 50, Loss: 0.268989235162735, Accuracy: 94.45880126953125\n","Epoch 51, Loss: 0.26114898920059204, Accuracy: 94.48621368408203\n","Epoch 52, Loss: 0.2569256126880646, Accuracy: 94.627197265625\n","Epoch 53, Loss: 0.25116419792175293, Accuracy: 94.69377136230469\n","Epoch 54, Loss: 0.24849587678909302, Accuracy: 94.7760009765625\n","Epoch 55, Loss: 0.24471838772296906, Accuracy: 94.92481231689453\n","Epoch 56, Loss: 0.24269209802150726, Accuracy: 95.02662658691406\n","Epoch 57, Loss: 0.2334165722131729, Accuracy: 95.19893646240234\n","Epoch 58, Loss: 0.22880110144615173, Accuracy: 95.28900146484375\n","Epoch 59, Loss: 0.22772587835788727, Accuracy: 95.29683685302734\n","Epoch 60, Loss: 0.2192421704530716, Accuracy: 95.48088836669922\n","Epoch 61, Loss: 0.21180613338947296, Accuracy: 95.5670394897461\n","Epoch 62, Loss: 0.2108718305826187, Accuracy: 95.77458953857422\n","Epoch 63, Loss: 0.20726417005062103, Accuracy: 95.82941436767578\n","Epoch 64, Loss: 0.2033306062221527, Accuracy: 95.9390640258789\n","Epoch 65, Loss: 0.19777724146842957, Accuracy: 96.03305053710938\n","Epoch 66, Loss: 0.19467845559120178, Accuracy: 96.18577575683594\n","Epoch 67, Loss: 0.1899964064359665, Accuracy: 96.24452209472656\n","Epoch 68, Loss: 0.1850137710571289, Accuracy: 96.35808563232422\n","Epoch 69, Loss: 0.1814829260110855, Accuracy: 96.40507507324219\n","Epoch 70, Loss: 0.17891870439052582, Accuracy: 96.51472473144531\n","Epoch 71, Loss: 0.1733604073524475, Accuracy: 96.61653900146484\n","Epoch 72, Loss: 0.16837038099765778, Accuracy: 96.74968719482422\n","Epoch 73, Loss: 0.16494369506835938, Accuracy: 96.80842590332031\n","Epoch 74, Loss: 0.15987618267536163, Accuracy: 96.91807556152344\n","Epoch 75, Loss: 0.1563069075345993, Accuracy: 97.03947448730469\n","Epoch 76, Loss: 0.1528063863515854, Accuracy: 97.12171173095703\n","Epoch 77, Loss: 0.1511906087398529, Accuracy: 97.16087341308594\n","Epoch 78, Loss: 0.14563049376010895, Accuracy: 97.29010009765625\n","Epoch 79, Loss: 0.14104686677455902, Accuracy: 97.35275268554688\n","Epoch 80, Loss: 0.139926940202713, Accuracy: 97.43498992919922\n","Epoch 81, Loss: 0.13508504629135132, Accuracy: 97.47415161132812\n","Epoch 82, Loss: 0.1329285353422165, Accuracy: 97.48590087890625\n","Epoch 83, Loss: 0.12985175848007202, Accuracy: 97.63471221923828\n","Epoch 84, Loss: 0.12635068595409393, Accuracy: 97.6190414428711\n","Epoch 85, Loss: 0.12366726994514465, Accuracy: 97.72087097167969\n","Epoch 86, Loss: 0.11994814872741699, Accuracy: 97.80702209472656\n","Epoch 87, Loss: 0.11804389208555222, Accuracy: 97.85401153564453\n","Epoch 88, Loss: 0.11573857814073563, Accuracy: 97.9088363647461\n","Epoch 89, Loss: 0.11315867304801941, Accuracy: 97.9010009765625\n","Epoch 90, Loss: 0.11013976484537125, Accuracy: 97.98323822021484\n","Epoch 91, Loss: 0.10717929899692535, Accuracy: 98.06156158447266\n","Epoch 92, Loss: 0.10398887097835541, Accuracy: 98.08113861083984\n","Epoch 93, Loss: 0.10238948464393616, Accuracy: 98.14771270751953\n","Epoch 94, Loss: 0.09933750331401825, Accuracy: 98.15946197509766\n","Epoch 95, Loss: 0.0968446284532547, Accuracy: 98.21037292480469\n","Epoch 96, Loss: 0.09512742608785629, Accuracy: 98.20645141601562\n","Epoch 97, Loss: 0.09221653640270233, Accuracy: 98.25736236572266\n","Epoch 98, Loss: 0.0906682014465332, Accuracy: 98.28868865966797\n","Epoch 99, Loss: 0.08845733106136322, Accuracy: 98.33568572998047\n","Epoch 100, Loss: 0.08696258813142776, Accuracy: 98.37092590332031\n","Epoch 101, Loss: 0.08557437360286713, Accuracy: 98.3787612915039\n","Epoch 102, Loss: 0.08286464959383011, Accuracy: 98.39051055908203\n","Epoch 103, Loss: 0.08125446736812592, Accuracy: 98.43358612060547\n","Epoch 104, Loss: 0.07933172583580017, Accuracy: 98.47274780273438\n","Epoch 105, Loss: 0.07739802449941635, Accuracy: 98.4962387084961\n","Epoch 106, Loss: 0.07565692067146301, Accuracy: 98.55106353759766\n","Epoch 107, Loss: 0.0741131454706192, Accuracy: 98.56672668457031\n","Epoch 108, Loss: 0.07290922105312347, Accuracy: 98.5745620727539\n","Epoch 109, Loss: 0.07110343873500824, Accuracy: 98.61763763427734\n","Epoch 110, Loss: 0.07010062038898468, Accuracy: 98.60980224609375\n","Epoch 111, Loss: 0.06786651909351349, Accuracy: 98.66462707519531\n","Epoch 112, Loss: 0.06637571007013321, Accuracy: 98.6920394897461\n","Epoch 113, Loss: 0.06516621261835098, Accuracy: 98.71553802490234\n","Epoch 114, Loss: 0.06386929750442505, Accuracy: 98.71945190429688\n","Epoch 115, Loss: 0.06210857629776001, Accuracy: 98.79386138916016\n","Epoch 116, Loss: 0.060931626707315445, Accuracy: 98.83301544189453\n","Epoch 117, Loss: 0.0591140054166317, Accuracy: 98.84085083007812\n","Epoch 118, Loss: 0.05798949673771858, Accuracy: 98.87217712402344\n","Epoch 119, Loss: 0.05642266944050789, Accuracy: 98.85651397705078\n","Epoch 120, Loss: 0.05510862544178963, Accuracy: 98.88392639160156\n","Epoch 121, Loss: 0.0539238415658474, Accuracy: 98.95441436767578\n","Epoch 122, Loss: 0.05261915922164917, Accuracy: 98.95050048828125\n","Epoch 123, Loss: 0.05164610967040062, Accuracy: 98.97791290283203\n","Epoch 124, Loss: 0.049434028565883636, Accuracy: 99.03665161132812\n","Epoch 125, Loss: 0.04814654961228371, Accuracy: 99.07972717285156\n","Epoch 126, Loss: 0.04718836769461632, Accuracy: 99.06797790527344\n","Epoch 127, Loss: 0.04604164510965347, Accuracy: 99.11105346679688\n","Epoch 128, Loss: 0.0447906032204628, Accuracy: 99.11497497558594\n","Epoch 129, Loss: 0.04309545084834099, Accuracy: 99.21678924560547\n","Epoch 130, Loss: 0.04225720837712288, Accuracy: 99.18937683105469\n","Epoch 131, Loss: 0.041951458901166916, Accuracy: 99.20503997802734\n","Epoch 132, Loss: 0.040226638317108154, Accuracy: 99.24420928955078\n","Epoch 133, Loss: 0.03888380154967308, Accuracy: 99.25203704833984\n","Epoch 134, Loss: 0.03819048032164574, Accuracy: 99.27944946289062\n","Epoch 135, Loss: 0.03662577643990517, Accuracy: 99.34993743896484\n","Epoch 136, Loss: 0.035267382860183716, Accuracy: 99.37734985351562\n","Epoch 137, Loss: 0.03459826484322548, Accuracy: 99.35777282714844\n","Epoch 138, Loss: 0.03361772000789642, Accuracy: 99.38518524169922\n","Epoch 139, Loss: 0.03256847709417343, Accuracy: 99.45567321777344\n","Epoch 140, Loss: 0.031490132212638855, Accuracy: 99.42826080322266\n","Epoch 141, Loss: 0.03012099117040634, Accuracy: 99.45958709716797\n","Epoch 142, Loss: 0.029361195862293243, Accuracy: 99.50658416748047\n","Epoch 143, Loss: 0.028713136911392212, Accuracy: 99.52616119384766\n","Epoch 144, Loss: 0.028613265603780746, Accuracy: 99.49091339111328\n","Epoch 145, Loss: 0.02734282985329628, Accuracy: 99.54182434082031\n","Epoch 146, Loss: 0.02633790485560894, Accuracy: 99.57315063476562\n","Epoch 147, Loss: 0.02508547529578209, Accuracy: 99.57707214355469\n","Epoch 148, Loss: 0.024374010041356087, Accuracy: 99.63189697265625\n","Epoch 149, Loss: 0.02362963557243347, Accuracy: 99.63189697265625\n","Epoch 150, Loss: 0.02241262048482895, Accuracy: 99.6671371459961\n","Epoch 151, Loss: 0.02206474356353283, Accuracy: 99.67888641357422\n","Epoch 152, Loss: 0.020876210182905197, Accuracy: 99.70238494873047\n","Epoch 153, Loss: 0.02054617553949356, Accuracy: 99.69454956054688\n","Epoch 154, Loss: 0.019532766193151474, Accuracy: 99.72979736328125\n","Epoch 155, Loss: 0.019344957545399666, Accuracy: 99.70238494873047\n","Epoch 156, Loss: 0.01845049113035202, Accuracy: 99.7454605102539\n","Epoch 157, Loss: 0.019497212022542953, Accuracy: 99.71412658691406\n","Epoch 158, Loss: 0.020616091787815094, Accuracy: 99.67888641357422\n","Epoch 159, Loss: 0.01850185915827751, Accuracy: 99.706298828125\n","Epoch 160, Loss: 0.01712387055158615, Accuracy: 99.76112365722656\n","Epoch 161, Loss: 0.016284843906760216, Accuracy: 99.78462219238281\n","Epoch 162, Loss: 0.015387963503599167, Accuracy: 99.78070068359375\n","Epoch 163, Loss: 0.014551977626979351, Accuracy: 99.80811309814453\n","Epoch 164, Loss: 0.014038456603884697, Accuracy: 99.82377624511719\n","Epoch 165, Loss: 0.013440688140690327, Accuracy: 99.84727478027344\n","Epoch 166, Loss: 0.013059095479547977, Accuracy: 99.83161163330078\n","Epoch 167, Loss: 0.012758825905621052, Accuracy: 99.83552551269531\n","Epoch 168, Loss: 0.012492289766669273, Accuracy: 99.85902404785156\n","Epoch 169, Loss: 0.011809036135673523, Accuracy: 99.85118865966797\n","Epoch 170, Loss: 0.01153835840523243, Accuracy: 99.87468719482422\n","Epoch 171, Loss: 0.011404717341065407, Accuracy: 99.87077331542969\n","Epoch 172, Loss: 0.010936290957033634, Accuracy: 99.85902404785156\n","Epoch 173, Loss: 0.010727211833000183, Accuracy: 99.86685180664062\n","Epoch 174, Loss: 0.01056777872145176, Accuracy: 99.87468719482422\n","Epoch 175, Loss: 0.010319767519831657, Accuracy: 99.88252258300781\n","Epoch 176, Loss: 0.010104001499712467, Accuracy: 99.88643646240234\n","Epoch 177, Loss: 0.009871979244053364, Accuracy: 99.88643646240234\n","Epoch 178, Loss: 0.00927323754876852, Accuracy: 99.88643646240234\n","Epoch 179, Loss: 0.008869319222867489, Accuracy: 99.90601348876953\n","Epoch 180, Loss: 0.008661184459924698, Accuracy: 99.91384887695312\n","Epoch 181, Loss: 0.008462203666567802, Accuracy: 99.8942642211914\n","Epoch 182, Loss: 0.008093823678791523, Accuracy: 99.902099609375\n","Epoch 183, Loss: 0.007871492765843868, Accuracy: 99.91384887695312\n","Epoch 184, Loss: 0.00756315840408206, Accuracy: 99.902099609375\n","Epoch 185, Loss: 0.007520464714616537, Accuracy: 99.91776275634766\n","Epoch 186, Loss: 0.0074401251040399075, Accuracy: 99.91776275634766\n","Epoch 187, Loss: 0.007241415791213512, Accuracy: 99.91776275634766\n","Epoch 188, Loss: 0.007007644511759281, Accuracy: 99.91776275634766\n","Epoch 189, Loss: 0.006760656367987394, Accuracy: 99.89818572998047\n","Epoch 190, Loss: 0.0065812040120363235, Accuracy: 99.92167663574219\n","Epoch 191, Loss: 0.00660338718444109, Accuracy: 99.91776275634766\n","Epoch 192, Loss: 0.006580316461622715, Accuracy: 99.90601348876953\n","Epoch 193, Loss: 0.006324272137135267, Accuracy: 99.91776275634766\n","Epoch 194, Loss: 0.006039157044142485, Accuracy: 99.92559814453125\n","Epoch 195, Loss: 0.005859787575900555, Accuracy: 99.902099609375\n","Epoch 196, Loss: 0.005839608609676361, Accuracy: 99.92167663574219\n","Epoch 197, Loss: 0.005603492259979248, Accuracy: 99.92951202392578\n","Epoch 198, Loss: 0.005613507702946663, Accuracy: 99.92559814453125\n","Epoch 199, Loss: 0.005374979693442583, Accuracy: 99.92559814453125\n","Epoch 200, Loss: 0.00523389782756567, Accuracy: 99.91776275634766\n"]}]},{"cell_type":"markdown","source":["## 테스트 루프 작동 시키기"],"metadata":{"id":"kgNN6xEaZEZ3"}},{"cell_type":"code","source":["for test_seq, test_labels in test_ds:\n","    prediction = test_step(model, test_seq)\n","    test_text = tokenizer.sequences_to_texts(test_seq.numpy())\n","    gt_text = tokenizer.sequences_to_texts(test_labels.numpy())\n","    texts = tokenizer.sequences_to_texts(prediction.numpy())\n","    print('_')\n","    print('q: ', test_text[0].strip())\n","    print('a: ', gt_text[0].strip())\n","    print('p: ', texts[0].strip())"],"metadata":{"id":"-6VJVTGNZGZ4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b6d1ce0a-fe39-4f00-fe49-7187428514a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["_\n","q:  여기 기프티콘 되죠\n","a:  네 현금영수증 해드릴까 요\n","p:  아메리카노 티라미슈 세트 가격 은 만 원 입니다\n","_\n","q:  네 에 테이크 아웃 도 가능한가요\n","a:  네 로 오시 면 테이크 아웃 잔 에 담아 드려요\n","p:  아뇨 현재 법적 으로 금지 하고 있어요\n","_\n","q:  아메리카노 톨 사이즈 로 주세요\n","a:  따뜻한 거 로 드릴 까요\n","p:  다른 건 필요 없으신 가요\n","_\n","q:  진동 을 따로 주시나요\n","a:  주 번호 로 드리겠습니다\n","p:  네 초코 머핀 이랑 치즈케이크 있습니다\n","_\n","q:  자리 있나요\n","a:  네 있습니다\n","p:  네 자리 있습니다\n","_\n","q:  그럼 루이보스 밀크 티 하나\n","a:  네 알겠습니다\n","p:  여기 진동 벨 가지 고 계시다가 울리면 주문 한 음료 가져가세요\n","_\n","q:  다음 에 무료 로 하고 엔 도장 찍어주세요\n","a:  네\n","p:  네 고객 님 결제 완료 되었습니다\n","_\n","q:  아메리카노 한 잔 에 얼마 죠\n","a:  입니다\n","p:  4000원 입니다\n","_\n","q:  얼마나\n","a:  바로 만들어 드릴게요\n","p:  오늘 딱 맞게 오셨네요\n","_\n","q:  카푸치노 는 로 주시 고 아메리카노 는 로\n","a:  네 더 없으세요\n","p:  결제 완료 되었습니다\n","_\n","q:  아메리카노 는 어떤 종류 가 있나요\n","a:  디카 페인 과 기본 아메리카노 2 종류 있습니다\n","p:  요즘 은 초코 케이크 가 제일 잘나가요\n","_\n","q:  카카오 페이 로 결제 가능한가요\n","a:  네 가능합니다\n","p:  네 가능합니다\n","_\n","q:  오늘 의 커피 는 커피 로 하나요 맛 이\n","a:  아 네 오늘 은 과테말라 커피 입니다\n","p:  오늘 딱 맞게 오셨네요\n","_\n","q:  머핀 은 뭐 가 제일\n","a:  블루베리 머핀 이 잘 나갑니다\n","p:  와이파이 비밀번호 는 종이 에 써져있습니다\n","_\n","q:  현금 영수증 해주세요\n","a:  네 번호 찍어주세요\n","p:  네 번호 찍어주세요\n","_\n","q:  둘 다 톨 사이즈 로 주세요\n","a:  여기 서 드시고 요\n","p:  주문 이 따뜻하게 드릴 까요\n","_\n","q:  아이스 아메리카노 한 잔 가능한가요\n","a:  네 가능합니다\n","p:  네 더 필요하신 건 없으세요\n","_\n","q:  아이스 아메리카노 에 샷 이 몇 개\n","a:  아이스 아메리카노 에 샷 은 개\n","p:  네 현금영수증 해드릴까 요\n","_\n","q:  카페라테 한 잔 주세요\n","a:  카페라테 따뜻한 걸 로 드릴 까요\n","p:  네 카페라떼 컵 사이즈 는 뭘 로 드릴 까요\n","_\n","q:  아니요\n","a:  네 더 필요하신 건 없으신 가요\n","p:  휘핑크림 올려 드릴 까요\n","_\n","q:  네 찍어주세요\n","a:  네 주문 딸기 스무디 와 쿠키 드릴게요\n","p:  더 필요한 건 없으시고요\n","_\n","q:  시즌 메뉴 오늘 도 가능한가요\n","a:  네 시즌 메뉴 가능합니다\n","p:  네 고객 님 티 종류 다 아이스 가능합니다\n","_\n","q:  시즌 메뉴 와 함께 되어 있는 세트 메뉴 가 있나요\n","a:  네 치즈 케이크 와 시즌 메뉴 두 잔 으로 세트 메뉴 있습니다\n","p:  네 그럼 바로 준비 해드리겠습니다\n","_\n","q:  라테 에 우유 두 도 변경 가능한가요\n","a:  네 라테 에 두유 로 변경 가능합니다\n","p:  기프티콘 으로 대체 할 수 있는 시즌 메뉴 는 없습니다\n","_\n","q:  네 먹고 갈 거 예요\n","a:  카드 여기 주세요\n","p:  포크 는 몇 개 드릴 까요\n","_\n","q:  카페인 이 음료 있나요\n","a:  티 음료 와 스무디 에는 카페인 이 않습니다\n","p:  아뇨 현재 법적 으로 금지 하고 있어요\n","_\n","q:  딸기스무디 랑 키위 스무디 는 생 과일 인가요\n","a:  딸기 는 키위 는 생 과일 을 사용 하고 있습니다\n","p:  500원 의 추가 비용 이 발생 되는데 괜찮으세요\n","_\n","q:  그럼 딸기 스무디 하나 주세요\n","a:  드시고 가시나요\n","p:  지금 은 아이스 가 안됩니다\n","_\n","q:  아메리카노 한 잔이요\n","a:  아이스 아메리카노 로 드릴 까요\n","p:  따뜻한 것 으로 드릴가요\n","_\n","q:  네 도 같이\n","a:  네 아메리카노 4000원 입니다\n","p:  네 카페라떼 컵 사이즈 는 뭘 로 드릴 까요\n","_\n","q:  디카 페인 아이스 아메리카노 한 잔 주세요\n","a:  디카 페인 아이스 아메리카노 는 기존 금액 에 300원 추가 되는데 괜찮으신 가요\n","p:  네 사이즈 는 어떤 걸 로 주문 넣어 드릴 까요\n","_\n","q:  커피 음료 것 뭐 가 있나요\n","a:  스무디 와 주스 있습니다\n","p:  티 와 주스 가 있습니다\n","_\n","q:  주스 어떤 종류 있나요\n","a:  딸기 주스 주스 주스 가 있습니다\n","p:  네 있습니다\n","_\n","q:  플랫 화이트 라지 로 주세요\n","a:  네\n","p:  플랫 화이트 사이즈 는 뭘 로 하시겠어요\n","_\n","q:  네 레드 벨벳 케이크 주세요\n","a:  음료 는 뭘 로 드릴 까요\n","p:  사이즈 선택 해주세요\n","_\n","q:  네 먹고 갈 거 예요\n","a:  유리잔 괜찮으세요\n","p:  포크 는 몇 개 드릴 까요\n","_\n","q:  따뜻한 밀크 티 주세요\n","a:  네\n","p:  샌드위치 는 안 필요하신가요\n","_\n","q:  음료 얼마나 하나요\n","a:  10분 정도 주시 면 됩니다\n","p:  오늘 은 다 팔렸어요\n","_\n","q:  아이스 아메리카노 한잔 얼마 인가요\n","a:  4500원 입니다\n","p:  500원 추가 하면 가능합니다\n","_\n","q:  현금영수증 번호\n","a:  네\n","p:  대략 5분 정도 걸립니다\n","_\n","q:  이 카드 로 결제 해주세요\n","a:  네 결제 도 와 드릴게요\n","p:  네 감사합니다\n","_\n","q:  주문 한 게 다 안\n","a:  주 번호 가 몇 이 죠\n","p:  네 계산 도 와 드릴게요\n","_\n","q:  을\n","a:  \n","p:  죄송합니다만 페퍼민트 는 방금 전 에 다 떨어졌어요\n","_\n","q:  베이글 은 얼마 인가요\n","a:  베이글 은 2000원 입니다\n","p:  5천 원 입니다\n","_\n","q:  지금 되나요\n","a:  는 계절 메뉴 라 지금 은 판매 하지 않습니다\n","p:  네 배달 비 3000원 입니다\n","_\n","q:  바닐라 라테 는 따뜻하게 주세요\n","a:  네 적립 이나 할인 카드 있으세요\n","p:  사이즈 는 어떻게 드릴 까요\n","_\n","q:  테이크 아웃 으로 부탁드립니다\n","a:  결제 는 이 쪽 에서 도 와 드릴게요\n","p:  네 진동 벨 울리면 와주세요\n","_\n","q:  혹시 테이크 아웃 잔 에 수 있나요\n","a:  테이크 아웃 하시는 건가 요\n","p:  개인 컵 사이즈 가 작아서 음료 가 다 안 들어가요\n","_\n","q:  아메리카노 하나 는 샷 추가 해주세요\n","a:  아메리카노 는 둘 다 따뜻한 걸 로 드릴 까요\n","p:  네 저희 는 어떤 걸 로 하시겠어요\n","_\n","q:  쿠폰 찍어주세요\n","a:  네 찍어 드릴게요\n","p:  10 개 다모아 오시 면 커피한잔 드려요\n","_\n","q:  주문 할게요\n","a:  어떤 거 드릴 까요\n","p:  네 어떤 걸 로 하시겠습니까\n","_\n","q:  파나요\n","a:  는 계절 지금 은\n","p:  네 어떤 거 로 드릴 까요\n","_\n","q:  그럼 겨울 메뉴 뭐 가\n","a:  겨울 엔 감귤 라테 가 제일 많이 나가요\n","p:  요즘 은 초코 케이크 가 제일 잘나가요\n","_\n","q:  네 결제 는 카드 로 할게요\n","a:  네 결제 완료 되었습니다\n","p:  네 카드 받았습니다\n","_\n","q:  둘 다 사이즈 로 할게요\n","a:  네 결제 는 어떤 것 으로 도 와 드릴 까요\n","p:  500원 할인 300원 같이 해드릴게요\n","_\n","q:  기프티콘 으로 결제 할게요\n","a:  네 그럼 쿠폰 저\n","p:  네 유효 기간 내 라면 사용 가능하세요\n","_\n","q:  녹차 라테 1 잔 주세요\n","a:  따뜻한 걸 로 드릴 까요\n","p:  휘핑크림 올려 드릴 까요\n","_\n","q:  네 그럼 휘핑크림 추가 해서 주세요\n","a:  네 녹차 라테 에 휘핑크림 추가 해서 4500원 입니다\n","p:  네 저희 가게 마스코트 메뉴 에요\n","_\n","q:  브레드 종류 는 뭐 가 있나요\n","a:  허니 브레드 와 갈릭 치즈 브레드 가 있습니다\n","p:  과일 생크림 케이크 가 잘 나가요\n","_\n","q:  생크림 이 건 어떤 건가 요\n","a:  허니 브레드 입니다\n","p:  스콘 은 데워 드릴 까요\n","_\n","q:  네 그렇게 만들어 주세요\n","a:  더 필요한 건 없으세요\n","p:  네 진동 벨 울리면 찾으러 오세요\n","_\n","q:  여기 있습니다\n","a:  네 확인 되셨고 되면 진동 벨 거 예요\n","p:  드시고 가시나요\n","_\n","q:  핫초코 한 잔 아메리카노 사이 즈 업 한 잔 하면 얼마 인가요\n","a:  입니다\n","p:  아이스 로 드릴 까요\n","_\n","q:  주스 는 다른 건 없나요\n","a:  그럼 에 라테 추천\n","p:  네 그럼 바로 준비 해드리겠습니다\n","_\n","q:  그건\n","a:  네 만 따듯 해 요\n","p:  11시 까지 합니다\n","_\n","q:  통신사 할인 되죠\n","a:  네 300원 할인 됩니다\n","p:  네 드시고 가시나요\n","_\n","q:  매장 에서 언제 까지 영업 하시나요\n","a:  오후 10시 까지 영업 입니다\n","p:  네 기다리시면 안내 해드릴게요\n","_\n","q:  아니요 그냥 주세요\n","a:  결제 해드릴게요\n","p:  카페모카 5천 원 입니다\n","_\n","q:  가격 안 되나요\n","a:  한 해드릴게요\n","p:  네 배달 비 3000원 입니다\n","_\n","q:  카페라테 한잔 주세요\n","a:  따뜻한 걸 로 드릴 까요\n","p:  휘핑크림 올려 드릴 까요\n","_\n","q:  네 차가운 걸 로 주세요\n","a:  4500원 입니다\n","p:  드시고 가시나요\n","_\n","q:  어떤 게 괜찮아요\n","a:  이나 원두 를 하시는 게 아니면 예 가체 많이 추천\n","p:  루이보스 아쌈 이렇게 두 개 있습니다\n","_\n","q:  그럼 추천 치즈 케이크 도 같이 주세요\n","a:  네 매장 에서 드시고 가시나요\n","p:  네 쿠폰 찍어 드릴 까요\n","_\n","q:  그리고 휘핑크림 은 에스프레소 크림 으로\n","a:  결제 는 어떻게 해드릴까 요\n","p:  네 그럼 바로 준비 해드리겠습니다\n","_\n","q:  지금 도 할인 하나요\n","a:  네 10시 까지 하고 있습니다\n","p:  네 진동 벨 울리면 와주세요\n","_\n","q:  그럼 와 아이스 아메리카노 로 할게요\n","a:  더 필요하신 건 없나요\n","p:  네 더 필요한 거 없으신 가요\n","_\n","q:  네 할인 적립 은\n","a:  네 바코드\n","p:  네 카페라떼 컵 사이즈 는 뭘 로 드릴 까요\n","_\n","q:  초코 프라푸치노 주세요\n","a:  휘핑 올려 드릴 까요\n","p:  다른 사항 은 없으신 가요\n","_\n","q:  시럽 도 가\n","a:  드시고 가시나요\n","p:  현금영수증 필요하세요\n","_\n","q:  둘 다 사이즈 로 주세요\n","a:  드시고 가시나요\n","p:  따뜻하게 드릴 까요\n","_\n","q:  마시다가 갈 건데 테이크아웃 으로 주세요\n","a:  상 매장 에서는 머그컵 으로 드리고 있어요\n","p:  쿠폰 주시 면 도 와 드리겠습니다\n","_\n","q:  나갈 때 테이크아웃 컵 으로 수 있나요\n","a:  네 그렇게 해드릴게요\n","p:  개인 컵 사이즈 가 작아서 음료 가 다 안 들어가요\n","_\n","q:  아메리카노 두 잔 한잔 주세요\n","a:  드시고 가실 건가 요\n","p:  카페모카 도 아이스 맞으세요\n","_\n","q:  얼마나 하나요\n","a:  5분 정도\n","p:  11시 까지 합니다\n","_\n","q:  포인트 적립 해주세요\n","a:  네 번호 입력 부탁드립니다\n","p:  네 알겠습니다\n","_\n","q:  네 번호 로 할게요\n","a:  네 에 번호\n","p:  네 드시고 가시나요\n","_\n","q:  아 포인트 포인트 사용 해주세요\n","a:  네 고객 님 포인트 총 있으신 데 사용 도 와 드리겠습니다\n","p:  네 준비 해드리겠습니다\n","_\n","q:  톨 사이즈 로 주문 할게요\n","a:  네 계산 도 와 드리겠습니다\n","p:  아메리카노 따뜻한 건가 요\n","_\n","q:  아메리카노 사이즈 가능한가요\n","a:  네 500원 만 추가 하시면 가능하십니다\n","p:  물론 이 죠\n","_\n","q:  사이 즈 업 해서 주세요\n","a:  네 결제 는 어떻게 도 와 드릴 까요\n","p:  네 더 필요한 거 면 됩니다\n","_\n","q:  커피 는 텀블러 에 담아주세요\n","a:  네 텀블러 할인 4000원 결제 도 와 드리겠습니다\n","p:  텀블러 할인 300원 같이 해드릴게요\n","_\n","q:  아니요 아이스 로 주세요\n","a:  드시고 가실 건가 요\n","p:  레귤러 사이즈 로 괜찮으세요\n","_\n","q:  테이크아웃 할게요\n","a:  지금 중 인데 케이크 주문 하시면 아메리카노 한잔 로 드려요\n","p:  결제 완료 되셨고 영수증 여기 있습니다\n","_\n","q:  현금 결제 가 안\n","a:  현금 은 에서 주문 도 와 드리겠습니다\n","p:  결제 완료 되었습니다\n","_\n","q:  포인트 적립 되나요\n","a:  번호 포인트 적립 도 와 드리고 있어요\n","p:  네\n","_\n","q:  포인트 적립 할게요\n","a:  네 결제 되셨습니다\n","p:  네 알겠습니다\n","_\n","q:  티라미수 는 있나요\n","a:  네 티라미수 는 있습니다\n","p:  네 500원 할인 되세요\n","_\n","q:  네 현금영수증 해주세요\n","a:  네 드시고 가시나요\n","p:  알겠습니다\n","_\n","q:  샷 추가 해주세요\n","a:  네 알겠습니다\n","p:  네 앞 에 카드 꽃 아 주시 면 됩니다\n","_\n","q:  얼마 에요\n","a:  만 원 입니다\n","p:  5천 원 입니다\n","_\n","q:  아이스 아메리카노 랑 샌드위치 주세요\n","a:  10시 에 세트 할인 가능하세요\n","p:  할인 카드 도 안 하시나요\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"RDoy_FGMjtA-"},"execution_count":null,"outputs":[]}]}